In general: 
	The assumptions that were made when writing the three programs are the following:
	-The input integers are always positive, thus the highest number that could be processed is 32767.

For Part A:
	Design decisions:
		Due to the nature of the problem, the most efficient way of computing the problem is by keeping track of the
		min, max, and sum of integers as the integers are being read in. This avoids the need to store all the integers
		into an array, thus saving the time needed to allocate and reallocate memory for the array, and the time needed
		to traverse the array after already reading in all the integers.
		Integers are read in by opening the input text file into a file descriptor, reading the file descriptor one
		character at a time and adding that character into a character array of size 6, which means that the character 
		array can hold up to 5 characters along with the null terminating character needed to make the array a "string".
		When a new line is encountered, this array is passed into atoi() to get the corresponding integer, and comparisons
		are performed against the existing min and max.
		Nothing was particularly learned in this problem, as it only involved reading in a file.
	Timings:
		Input 10: 0.005s
		Input 100: 0.006s
		Input 1k: 0.009s
		Input 10k: 0.031s
		Input 100k: 0.185s
		
For Part B:
	Design decisions:
		This algorithm is very similar to part C's divide and conquer approach (but slightly modified).  In this design decision we use a skewed binary process tree due to the requirement of the assignment.
		Because this part was created seperately from part A and B, the design is slightly different.  The lines are read, and completely parsed, in the very beginning, in the parent process.  The numbers from the file are stored in an array of integers, reading each line one-by-one and using atoi() on each line.  The array of numbers is shared with the children processes through shared memory (shm).  This allows for all processes to refer to the same array of numbers without wasting any space.
		Upon forking, the array of numbers is split in half, the parent analyses the first half, in sequential order, for the minimum, maximum, and sum.  The second half is analyzed by the child process, which repeats the same process as the parent process.  It too cuts it's half in half, completing it's first half and sending the other half to the child process it creates.
		Upon the completion of wait, and reading the pipe established between the parent and child processes, the parent process compares the child min, and max with its own, and combines the two sums.  These numbers are then returned to it's parent.
	Timings:
		Input 10: 0.005s
		Input 100: 0.008s
		Input 1k: 0.011s
		Input 10k: 0.016s
		Input 100k: 0.036s
	
For Part C:
	Design decisions:
		The algorithm is a divide and conquer algorithm, where each process creates two processes, each handling the 
		left and right halves of the array containing all input integers. This division occurs until a process only
		has to handle at most two elements in the array. The process then computes the minimum and maximum integers, 
		along with their sum, and returns the min, max, and sum in a struct back to the parent, which performs the same
		comparisons until the first process has the total minimum, total maximum, and total sum. Normally this algorithm
		would best be implemented recursively, but using it with multiple processes allows for a nice binary process tree.
		The input file is read in the original parent process and copied over to shared memory. This is done so that there exists
		only one copy of the integer array at once as opposed to having to copy the array for each process each time a fork() is 
		performed. 
		Between each immediate parent and child, pipes are used to transmit the min, max, and sum which are contained in a struct 
		that is passed to the parent from the child.
	Timings:
		Input 10: 0.009s
		Input 100: 0.024s
		Input 1k: 0.158s
		Input 10k: 2.219s
		Input 100k: 6.970s
	Regarding limitation of process numbers:
		There does not appear to be a limit to the number of processes that are created. 
		For the input file size of 100k integers, there should theoretically be at least 10k processes 
		running at the same time, but this poses no issue and the program
		still runs successfully. The number of processes running at the same time could potentially be less however since 
		processes that were created earlier will terminate earlier due to having finished computation earlier. For a much
		greater input file length, there could potentially be a process limit due to limited system resources, but this was
		not encountered with the given input sizes for this problem.
		
Overall Comments:
	It is noted that the single process program (Part A) has the fastest run time for small input sizes whereas Part B has a faster
	run time for larger input sizes, which is expected for several reasons.
	The first reason is that the algorithm used for the single process program never has to allocate and reallocate memory, 
	since all the computation is done when reading in the file, so it is faster for smaller input sizes. 
	The second is that no new processes have to be generated for the single process program, which
	can incur a lot of overhead that contributes to a longer program run time. This factor is likely the main contributing factor
	to the longer runtime of the multiprocess program of Part C.
	The program of PartB is faster with larger input sizes due to the benefits of dividing the work outweighting the cost
	of needing to generate new processes. 
